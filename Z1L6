import numpy as np


def Sigma():
    def sigmoid(x):
        return 1.0 / (1 + np.exp(-x))

    def sigmoid_derivative(x):
        return x * (1.0 - x)

    class NeutralNetwork:
        def __init__(self, x, y):
            self.input = x
            self.weights1 = np.random.rand(4, self.input.shape[1])
            self.weights2 = np.random.rand(1, 4)
            self.y = y
            self.output = np.zeros(self.y.shape)
            self.eta = 2
            self.eta2 = 0.04

        def feedforward(self):
            self.layer1 = sigmoid(np.dot(self.input, self.weights1.T))
            self.output = sigmoid(np.dot(self.layer1, self.weights2.T))

        def backprop(self):
            delta2 = (self.y - self.output) * \
                     sigmoid_derivative(self.output)
            d_weights2 = self.eta * np.dot(delta2.T, self.layer1)
            delta1 = sigmoid_derivative(self.layer1) * \
                     np.dot(delta2, self.weights2)
            d_weights1 = self.eta * np.dot(delta1.T, self.input)
            self.weights1 += d_weights1
            self.weights2 += d_weights2

        def backprop2(self):
            delta2 = (self.y - self.output) * \
                     sigmoid_derivative(self.output)
            d_weights2 = self.eta2 * np.dot(delta2.T, self.layer1)
            delta1 = sigmoid_derivative(self.layer1) * \
                     np.dot(delta2, self.weights2)
            d_weights1 = self.eta2 * np.dot(delta1.T, self.input)
            self.weights1 += d_weights1
            self.weights2 += d_weights2

    X = np.array([

        [0, 0, 1],
        [0, 1, 1],
        [1, 0, 1],
        [1, 1, 1]

    ])
    y = np.array([

        [0], [1], [1], [0]

    ])
    y2 = np.array([

        [0], [1], [1], [1]

    ])
    y3 = np.array([
        [0], [0], [0], [1]

    ])
    nn = NeutralNetwork(X, y)
    nn2 = NeutralNetwork(X, y2)
    nn3 = NeutralNetwork(X, y3)
    for i in range(10000):
        nn.feedforward()
        nn.backprop()
        nn2.feedforward()
        nn3.feedforward()
        nn2.backprop()
        nn3.backprop2()
    print("Dla XOR:")
    print(nn.output)
    print("Dla OR:")
    print(nn2.output)
    print("Dla AND:")
    print(nn3.output)


def Relu():
    def relu(x):
        return x * (x > 0)

    def relu_derivative(x):
        return 1. * (x > 0)

    np.random.seed(17)

    class NeutralNetwork:
        def __init__(self, x, y):
            self.input = x
            self.weights1 = np.random.rand(4, self.input.shape[1])
            self.weights2 = np.random.rand(1, 4)
            self.y = y
            self.output = np.zeros(self.y.shape)
            self.eta = 0.001
            self.eta2 = 0.0009

        def feedforward(self):
            self.layer1 = relu(np.dot(self.input, self.weights1.T))
            self.output = relu(np.dot(self.layer1, self.weights2.T))

        def backprop(self):
            delta2 = (self.y - self.output) * \
                     relu_derivative(self.output)
            d_weights2 = self.eta * np.dot(delta2.T, self.layer1)
            delta1 = relu_derivative(self.layer1) * \
                     np.dot(delta2, self.weights2)
            d_weights1 = self.eta * np.dot(delta1.T, self.input)
            self.weights1 += d_weights1
            self.weights2 += d_weights2

        def backpropAnd(self):
            delta2 = (self.y - self.output) * \
                     relu_derivative(self.output)
            d_weights2 = self.eta2 * np.dot(delta2.T, self.layer1)
            delta1 = relu_derivative(self.layer1) * \
                     np.dot(delta2, self.weights2)
            d_weights1 = self.eta2 * np.dot(delta1.T, self.input)
            self.weights1 += d_weights1
            self.weights2 += d_weights2

        def backpropOR(self):
            delta2 = (self.y - self.output) * \
                     relu_derivative(self.output)
            d_weights2 = self.eta2 * np.dot(delta2.T, self.layer1)
            delta1 = relu_derivative(self.layer1) * \
                     np.dot(delta2, self.weights2)
            d_weights1 = self.eta2 * np.dot(delta1.T, self.input)
            self.weights1 += d_weights1
            self.weights2 += d_weights2

    X = np.array([

        [0, 0, 1],
        [0, 1, 1],
        [1, 0, 1],
        [1, 1, 1]

    ])
    y = np.array([

        [0], [1], [1], [0]

    ])
    y2 = np.array([

        [0], [1], [1], [1]

    ])
    y3 = np.array([
        [0], [0], [0], [1]

    ])
    nn = NeutralNetwork(X, y)
    nn2 = NeutralNetwork(X, y2)
    nn3 = NeutralNetwork(X, y3)
    for i in range(10000):
        nn.feedforward()
        nn.backprop()
        nn2.feedforward()
        nn3.feedforward()
        nn2.backpropOR()
        nn3.backpropAnd()
    print("Dla XOR:")
    print(nn.output)
    print("Dla OR:")
    print(nn2.output)
    print("Dla AND:")
    print(nn3.output)


def ReluSigma():
    def relu(x):
        return x * (x > 0)

    def relu_derivative(x):
        return 1. * (x > 0)

    def sigmoid(x):
        return 1.0 / (1 + np.exp(-x))

    def sigmoid_derivative(x):
        return x * (1.0 - x)

    np.random.seed(1)

    class NeutralNetwork:
        def __init__(self, x, y):
            self.input = x
            self.weights1 = np.random.rand(4, self.input.shape[1])
            self.weights2 = np.random.rand(1, 4)
            self.y = y
            self.output = np.zeros(self.y.shape)
            self.eta = 0.8
            self.eta2 = 0.01

        def feedforward(self):
            self.layer1 = relu(np.dot(self.input, self.weights1.T))
            self.output = sigmoid(np.dot(self.layer1, self.weights2.T))

        def backprop(self):
            delta2 = (self.y - self.output) * \
                     sigmoid_derivative(self.output)
            d_weights2 = self.eta * np.dot(delta2.T, self.layer1)
            delta1 = relu_derivative(self.layer1) * \
                     np.dot(delta2, self.weights2)
            d_weights1 = self.eta * np.dot(delta1.T, self.input)
            self.weights1 += d_weights1
            self.weights2 += d_weights2

        def backprop2(self):
            delta2 = (self.y - self.output) * \
                     sigmoid_derivative(self.output)
            d_weights2 = self.eta2 * np.dot(delta2.T, self.layer1)
            delta1 = relu_derivative(self.layer1) * \
                     np.dot(delta2, self.weights2)
            d_weights1 = self.eta2 * np.dot(delta1.T, self.input)
            self.weights1 += d_weights1
            self.weights2 += d_weights2

    X = np.array([

        [0, 0, 1],
        [0, 1, 1],
        [1, 0, 1],
        [1, 1, 1]

    ])
    y = np.array([

        [0], [1], [1], [0]

    ])
    y2 = np.array([

        [0], [1], [1], [1]

    ])
    y3 = np.array([
        [0], [0], [0], [1]

    ])
    nn = NeutralNetwork(X, y)
    nn2 = NeutralNetwork(X, y2)
    nn3 = NeutralNetwork(X, y3)
    for i in range(10000):
        nn.feedforward()
        nn.backprop()
        nn2.feedforward()
        nn3.feedforward()
        nn2.backprop2()
        nn3.backprop2()
    print("Dla XOR:")
    print(nn.output)
    print("Dla OR:")
    print(nn2.output)
    print("Dla AND:")
    print(nn3.output)


def SigmaRelu():
    def relu(x):
        return x * (x > 0)

    def relu_derivative(x):
        return 1. * (x > 0)

    def sigmoid(x):
        return 1.0 / (1 + np.exp(-x))

    def sigmoid_derivative(x):
        return x * (1.0 - x)

    np.random.seed(17)

    class NeutralNetwork:
        def __init__(self, x, y):
            self.input = x
            self.weights1 = np.random.rand(4, self.input.shape[1])
            self.weights2 = np.random.rand(1, 4)
            self.y = y
            self.output = np.zeros(self.y.shape)
            self.eta = 0.00621
            self.eta2 = 0.01
            self.eta3 = 0.015

        def feedforward(self):
            self.layer1 = sigmoid(np.dot(self.input, self.weights1.T))
            self.output = relu(np.dot(self.layer1, self.weights2.T))

        def backprop(self):
            delta2 = (self.y - self.output) * \
                     relu_derivative(self.output)
            d_weights2 = self.eta * np.dot(delta2.T, self.layer1)
            delta1 = sigmoid_derivative(self.layer1) * \
                     np.dot(delta2, self.weights2)
            d_weights1 = self.eta * np.dot(delta1.T, self.input)
            self.weights1 += d_weights1
            self.weights2 += d_weights2

        def backprop2(self):
            delta2 = (self.y - self.output) * \
                     relu_derivative(self.output)
            d_weights2 = self.eta2 * np.dot(delta2.T, self.layer1)
            delta1 = sigmoid_derivative(self.layer1) * \
                     np.dot(delta2, self.weights2)
            d_weights1 = self.eta2 * np.dot(delta1.T, self.input)
            self.weights1 += d_weights1
            self.weights2 += d_weights2

        def backprop3(self):
            delta2 = (self.y - self.output) * \
                     relu_derivative(self.output)
            d_weights2 = self.eta3 * np.dot(delta2.T, self.layer1)
            delta1 = sigmoid_derivative(self.layer1) * \
                     np.dot(delta2, self.weights2)
            d_weights1 = self.eta3 * np.dot(delta1.T, self.input)
            self.weights1 += d_weights1
            self.weights2 += d_weights2

    X = np.array([

        [0, 0, 1],
        [0, 1, 1],
        [1, 0, 1],
        [1, 1, 1]

    ])
    y = np.array([

        [0], [1], [1], [0]

    ])
    y2 = np.array([

        [0], [1], [1], [1]

    ])
    y3 = np.array([
        [0], [0], [0], [1]

    ])
    nn = NeutralNetwork(X, y)
    nn2 = NeutralNetwork(X, y2)
    nn3 = NeutralNetwork(X, y3)
    for i in range(10000):
        nn.feedforward()
        nn.backprop3()
        nn2.feedforward()
        nn3.feedforward()
        nn2.backprop2()
        nn3.backprop()
    print("Dla XOR:")
    print(nn.output)
    print("Dla OR:")
    print(nn2.output)
    print("Dla AND:")
    print(nn3.output)


while 1 == 1:
    x = (input("\nS (Sigma(wejście/wyjście),\n"
               "R (Relu wejście/wyjście),\n"
               "RS (Relu wejście, Sigma wyjście),\n"
               "SR (Sigma wejścia, Relu wyjście)\n"))
    if x == 'S' or 's':
        Sigma()
    elif x == 'R' or 'r':
        Relu()
    elif x == 'RS' or 'rs' or 'Rs' or 'rS':
        ReluSigma()
    elif x == 'SR' or 'sr' or 'Sr' or 'sR':
        SigmaRelu()
    else:
        print("Nie ma takiej funkcji")
